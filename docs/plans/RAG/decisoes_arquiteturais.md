# Decis√µes Arquiteturais - RAG Jur√≠dico BotSalinha

**Data:** 2025-02-28
**Status:** Milestone 0 e 1 completados, Milestone 2 em andamento

## √çndice

1. [Stack Tecnol√≥gico](#stack-tecnol√≥gico)
2. [Decis√µes de Design](#decis√µes-de-design)
3. [Trade-offs Analisados](#trade-offs-analisados)
4. [Problemas Resolvidos](#problemas-resolvidos)
5. [Decis√µes Pendentes](#decis√µes-pendentes)

---

## Stack Tecnol√≥gico

### Escolhas Confirmadas

| Componente | Tecnologia | Justificativa |
|-----------|------------|---------------|
| **Vector Store** | SQLite + √≠ndice customizado | Menos depend√™ncias, controle total, SQLite j√° em uso |
| **Embeddings** | OpenAI text-embedding-3-small | 1536 dim, $0.02/1M tokens, alta qualidade para pt-BR |
| **ORM** | SQLAlchemy 2.0 Async | J√° em uso no projeto, suporte async nativo |
| **Migra√ß√µes** | Alembic | Padr√£o de mercado para SQLAlchemy |
| **Parsing DOCX** | python-docx | Preserva formata√ß√£o e estrutura hier√°rquica |
| **Chunking** | Customizado (hier√°rquico) | Preserva estrutura jur√≠dica (artigo, par√°grafo, inciso) |
| **AI Orchestration** | Agno Framework | J√° em uso no projeto |
| **Valida√ß√£o** | Pydantic v2 | J√° em uso, type hints, valida√ß√£o autom√°tica |

### Tecnologias Consideradas e Rejeitadas

| Tecnologia | Motivo da Rejei√ß√£o |
|------------|-------------------|
| **ChromaDB** | Depend√™ncia adicional, SQLite suficiente para volume atual |
| **LangChain** | Agno j√° fornece orquestra√ß√£o, evita overhead |
| **sentence-transformers** | Requer GPU para bom desempenho, OpenAI API mais simples |
| **LlamaIndex** | Muito complexo para caso de uso atual |
| **pgvector** | Requer PostgreSQL, SQLite j√° em uso |

---

## Decis√µes de Design

### 1. Estrutura de Metadados Jur√≠dicos

**Decis√£o:** Metadados hier√°rquicos com campos especializados

```python
class ChunkMetadata(BaseModel):
    documento: str           # Nome do documento fonte
    titulo: str | None       # T√≠tulo principal (Livro, T√≠tulo)
    capitulo: str | None     # Cap√≠tulo
    secao: str | None        # Se√ß√£o
    artigo: str | None       # N√∫mero do artigo (ex: "37")
    paragrafo: str | None    # Par√°grafo √∫nico (ex: "1")
    inciso: str | None       # Inciso romano (ex: "I", "II")
    marca_atencao: bool      # #Aten√ß√£o: no texto
    marca_stf: bool          # #STF: no texto
    marca_stj: bool          # #STJ: no texto
    banca: str | None        # Banca de concurso (ex: "CEBRASPE")
    ano: str | None          # Ano da quest√£o
```

**Justificativa:**
- Preserva estrutura hier√°rquica de documentos jur√≠dicos
- Permite navega√ß√£o precisa (citar artigo espec√≠fico)
- Suporta quest√µes de concurso com metadados de banca/ano

---

### 2. Estrat√©gia de Chunking

**Decis√£o:** Chunking hier√°rquico com overlap de 50 tokens

```python
CHUNK_CONFIG = {
    "max_tokens": 500,           # Tamanho m√°ximo do chunk
    "overlap_tokens": 50,        # Overlap para contexto
    "respect_boundaries": True,  # Respeita limites de artigos
    "min_chunk_size": 100,       # Tamanho m√≠nimo
    "metadata_max_depth": 3,     # Profundidade de metadados a preservar
}
```

**Justificativa:**
- 500 tokens = bom balan√ßo contexto/precis√£o
- Overlap de 50 tokens mant√©m coes√£o entre chunks
- Respeitar limites de artigos evita cortar cita√ß√µes legais
- Min 100 tokens evita chunks fragmentados

**Trade-off:**
- ‚úÖ Preserva estrutura jur√≠dica
- ‚ùå Mais complexo que chunking por tamanho fixo

---

### 3. Batching Din√¢mico para Embeddings

**Decis√£o:** Limite de 200K tokens por request (margem para 300K m√°ximo)

```python
MAX_TOKENS_PER_REQUEST = 200000  # OpenAI max: 300K

if total_tokens <= MAX_TOKENS_PER_REQUEST:
    # Single request
else:
    # Process in batches, tracking tokens per batch
```

**Justificativa:**
- OpenAI text-embedding-3-small tem limite de 300K tokens
- Usar 200K como margem de seguran√ßa para erros de estimativa
- Estimativa `len(text) // 4` pode ser imprecisa
- Documentos grandes (CF/88 com 312K tokens) falharam sem batching

**Problema Resolvido:**
- CF/88 (312K tokens) falhou com "max_tokens_per_request"
- Solu√ß√£o: Batching din√¢mico divide em m√∫ltiplas requests

---

### 4. Configura√ß√£o com Pydantic-Settings

**Decis√£o:** Nested configs com `env_nested_delimiter="__"`

```python
class RAGConfig(BaseSettings):
    model_config = SettingsConfigDict(
        env_nested_delimiter="__",
        env_file=".env",
        env_file_encoding="utf-8",
    )
    enabled: bool = True
    top_k: int = 5
    min_similarity: float = 0.6
    embedding_model: str = "text-embedding-3-small"
```

**Vari√°veis de ambiente:**
```bash
RAG__ENABLED=true
RAG__TOP_K=5
RAG__EMBEDDING_MODEL=text-embedding-3-small
```

**Problema Resolvido:**
- Nested classes n√£o herdavam `env_file` automaticamente
- Solu√ß√£o: Adicionar `env_file=".env"` explicitamente

---

### 5. Eventos de Log em pt-BR

**Decis√£o:** Todos os nomes de eventos em portugu√™s

```python
LogEvents = {
    "rag_documento_indexado": "Documento indexado com sucesso",
    "rag_chunk_criado": "Chunk criado",
    "rag_embedding_criado": "Embedding gerado",
    "rag_busca_executada": "Busca vetorial executada",
    # ...
}
```

**Justificativa:**
- Consist√™ncia com restante do projeto
- Facilita debugging para time brasileiro
- Ferramentas de log (Datadog, CloudWatch) suportam unicode

---

## Trade-offs Analisados

### 1. SQLite vs ChromaDB

| Aspecto | SQLite | ChromaDB |
|---------|--------|----------|
| Depend√™ncias | 0 (j√° em uso) | +1 |
| Setup | Nenhum | Docker/service |
| Performance | ~10ms/busca | ~5ms/busca |
| Escalabilidade | at√© 1M chunks | at√© 100M chunks |
| Flexibilidade | Total | Limitada |

**Decis√£o:** SQLite
**Justificativa:**
- Volume atual < 100K chunks
- Zero depend√™ncias adicionais
- Controle total sobre implementa√ß√£o
- F√°cil migrar para ChromaDB se necess√°rio

---

### 2. OpenAI API vs Local Embeddings

| Aspecto | OpenAI API | Local (sentence-transformers) |
|---------|------------|-------------------------------|
| Custo | $0.02/1M tokens | Gr√°tis |
| Qualidade pt-BR | Alta | M√©dia |
| Lat√™ncia | ~100ms | ~500ms (CPU) |
| Setup | API key | Modelo 500MB+ |
| GPU | N√£o necess√°ria | Recomendada |

**Decis√£o:** OpenAI API
**Justificativa:**
- Qualidade superior para portugu√™s jur√≠dico
- Custo baixo ($0.60 por 1M chunks)
- Sem overhead de setup
- Lat√™ncia aceit√°vel

---

### 3. Chunking Hier√°rquico vs Tamanho Fixo

| Aspecto | Hier√°rquico | Tamanho Fixo |
|---------|-------------|--------------|
| Complexidade | Alta | Baixa |
| Preserva estrutura | ‚úÖ Sim | ‚ùå N√£o |
| Overhead metadados | Alto | Baixo |
| Precis√£o cita√ß√£o | ‚úÖ Alta | Baixa |

**Decis√£o:** Hier√°rquico
**Justificativa:**
- Caso de uso jur√≠dico requer precis√£o
- Citar artigo espec√≠fico √© obrigat√≥rio
- Overhead aceit√°vel para volume atual

---

## Problemas Resolvidos

### Problema 1: Alembic Async Driver

**Erro:** `The asyncio extension requires an async driver`

**Causa:** `alembic revision --autogenerate` n√£o suporta async SQLAlchemy

**Solu√ß√£o:**
```bash
# Criar migration manualmente
alembic revision -m "add_rag_tables"

# Escrever upgrade/downgrade manualmente
def upgrade():
    op.create_table("rag_documents", ...)
    op.create_table("rag_chunks", ...)
```

---

### Problema 2: API Key N√£o Lida do .env

**Erro:** `settings.openai.api_key` sempre retorna `None`

**Causa:** Nested Pydantic classes com `default_factory` n√£o herdam `env_file`

**Solu√ß√£o 1 (configura√ß√£o):**
```python
class OpenAIConfig(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",  # Adicionar explicitamente
        env_file_encoding="utf-8",
    )
```

**Solu√ß√£o 2 (workaround CLI):**
```python
api_key = os.environ.get("OPENAI_API_KEY") or os.environ.get("OPENAI__API__KEY")
if not api_key:
    api_key = settings.get_openai_api_key()
```

---

### Problema 3: Limite de Tokens OpenAI Excedido

**Erro:** `max_tokens_per_request exceeded` (CF/88 com 312K tokens)

**Causa:** Documento excede limite de 300K tokens da OpenAI

**Solu√ß√£o:** Batching din√¢mico em `EmbeddingService.embed_batch()`
```python
MAX_TOKENS_PER_REQUEST = 200000  # Margem de seguran√ßa

if total_tokens <= MAX_TOKENS_PER_REQUEST:
    # Single request
else:
    # Process in batches, tracking tokens per batch
    for idx, text in texts:
        if current_tokens + text_tokens > MAX_TOKENS_PER_REQUEST:
            # Process current batch, start new one
```

---

## Decis√µes Pendentes

### 1. Algoritmo de Similaridade

**Op√ß√µes:**
- Cosine similarity (padr√£o, perform√°tico)
- Dot product (mais r√°pido, requer vetores normalizados)
- Euclidean distance (mais lento)

**Decis√£o:** Come√ßar com cosine, avaliar outros se necess√°rio

---

### 2. Re-ranking de Resultados

**Op√ß√µes:**
- Sem re-ranking (simples)
- Re-ranking por relev√¢ncia jur√≠dica (artigo mais relevante que nota)
- Re-ranking com LLM (custoso, mas mais preciso)

**Decis√£o:** Implementar sem re-ranking inicial, adicionar re-ranking jur√≠dico se necess√°rio

---

### 3. Cache de Embeddings

**Op√ß√µes:**
- Sem cache (recriar a cada ingest√£o)
- Cache em mem√≥ria (fast, mas volatile)
- Cache persistente (SQLite, Redis)

**Decis√£o:** SQLite j√° armazena embeddings, cache adicional n√£o necess√°rio inicialmente

---

### 4. Atualiza√ß√£o de Documentos

**Op√ß√µes:**
- Deletar e reindexar (simples)
- Upsert por chunk_id (complexo)
- Versionamento de documentos (mais complexo)

**Decis√£o:** Deletar e reindexar para MVP, avaliar upsert se performance for problema

---

## M√©tricas de Sucesso

### Milestone 0 e 1: ‚úÖ Completados

- ‚úÖ 134 testes passando
- ‚úÖ 2 documentos indexados (775 chunks, 345K tokens)
- ‚úÖ Ingest√£o CF/88 (687 chunks, 303K tokens) com batching
- ‚úÖ Zero erros de encoding

### Milestone 2: Em andamento

- ‚è≥ Busca vetorial funcional
- ‚è≥ Testes E2E de busca

### MVP Completo

- üéØ Busca com similaridade > 0.7 retorna resultados relevantes
- üéØ Lat√™ncia de busca < 100ms
- üéØ Cobertura de 95% dos chunks em consultas jur√≠dicas comuns

---

## Refer√™ncias

- [Plano Principal](../../../.omc/plans/rag-feature-implementation.md)
- [Melhorias Sugeridas](./melhorias_sugeridas.md)
- [CLAUDE.md](../../../CLAUDE.md)
