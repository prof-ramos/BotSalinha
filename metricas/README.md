# BotSalinha M√©tricas

Este diret√≥rio cont√©m scripts para gera√ß√£o e an√°lise de m√©tricas de performance e qualidade do BotSalinha.

## üìÅ Estrutura de Diret√≥rios

```
metricas/
‚îú‚îÄ‚îÄ config.py                    # Configura√ß√£o centralizada (Pydantic)
‚îú‚îÄ‚îÄ utils.py                     # Fun√ß√µes utilit√°rias organizadas por categoria
‚îú‚îÄ‚îÄ base_metric.py               # Classe base abstrata para scripts de m√©trica
‚îú‚îÄ‚îÄ html_generator.py            # Gerador de HTML com Jinja2
‚îú‚îÄ‚îÄ run_all_metrics.py           # Script consolidado
‚îú‚îÄ‚îÄ gerar_performance.py         # M√©tricas end-to-end
‚îú‚îÄ‚îÄ gerar_performance_acesso.py  # M√©tricas de acesso ao banco
‚îú‚îÄ‚îÄ gerar_performance_rag.py     # M√©tricas de componentes RAG
‚îú‚îÄ‚îÄ gerar_qualidade.py           # M√©tricas de qualidade RAG
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ report.css               # CSS externo (tema jur√≠dico, WCAG AA)
‚îú‚îÄ‚îÄ templates/                   # Templates Jinja2
‚îÇ   ‚îú‚îÄ‚îÄ base.html                # Template base
‚îÇ   ‚îú‚îÄ‚îÄ summary.html             # Componente de sum√°rio
‚îÇ   ‚îú‚îÄ‚îÄ section.html             # Componente de se√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ charts.html              # Componente de gr√°ficos
‚îÇ   ‚îî‚îÄ‚îÄ report.html              # Template principal
‚îî‚îÄ‚îÄ tests/                       # Testes unit√°rios
    ‚îú‚îÄ‚îÄ conftest.py              # Fixtures pytest
    ‚îú‚îÄ‚îÄ test_config.py           # Tests de config.py
    ‚îî‚îÄ‚îÄ test_utils.py            # Tests de utils.py
```

## üèóÔ∏è Nova Arquitetura

### M√≥dulos Principais

#### `config.py`
Configura√ß√£o centralizada com Pydantic:
- Constantes centralizadas (thresholds, paths, timeouts)
- Type hints e valida√ß√£o
- Singleton pattern para cache

```python
from metricas.config import get_metrics_config

config = get_metrics_config()
print(config.rag_min_similarity)  # 0.4
print(config.script_timeout_seconds)  # 300
```

#### `html_generator.py`
Gerador de HTML usando Jinja2:
- Separa√ß√£o de dados e apresenta√ß√£o
- Templates reutiliz√°veis
- Suporte a gr√°ficos customiz√°veis

```python
from metricas.html_generator import get_html_generator

generator = get_html_generator()
generator.generate_report(results, csv_data, metadata, output_path)
```

#### `base_metric.py`
Classe base abstrata para scripts de m√©trica:
- Interface padronizada
- Setup, coleta, salvamento autom√°ticos
- Reduz duplica√ß√£o de c√≥digo

```python
from metricas.base_metric import BaseMetric

class MyMetric(BaseMetric):
    async def collect(self, **kwargs):
        # Coleta de dados
        return results
```

#### `utils.py`
Fun√ß√µes utilit√°rias organizadas por categoria:
- **Logging**: configure_logging, get_logger
- **CLI**: get_base_parser
- **CSV**: save_results_csv, save_summary_csv, load_csv, read_csv_dict
- **HTML**: escape_html, generate_html_table
- **Time**: format_duration, format_timestamp, Timer
- **Stats**: calculate_stats, format_percentile
- **Display**: print_summary_box, print_progress

### Sistema de Templates

Templates Jinja2 modulares no diret√≥rio `templates/`:
- `base.html`: Estrutura HTML base
- `summary.html`: Cards de sum√°rio
- `section.html`: Se√ß√µes de m√©tricas
- `charts.html`: Gr√°ficos de barras
- `report.html`: Relat√≥rio consolidado

### CSS Externo

Arquivo `static/report.css` com:
- Vari√°veis CSS customiz√°veis
- Tema jur√≠dico (dark com gold accents)
- WCAG AA compliant (contraste melhorado)
- Anima√ß√µes suaves
- Responsivo (mobile-friendly)
- Suporte a prefers-reduced-motion

### Testes Unit√°rios

Diret√≥rio `tests/` com:
- `test_config.py`: Testes de configura√ß√£o
- `test_utils.py`: Testes de utilit√°rios
- `conftest.py`: Fixtures pytest
- Meta: >=70% cobertura

## üöÄ Uso R√°pido

### Executar um script individual

```bash
# Usar configura√ß√µes padr√£o
uv run python metricas/gerar_performance.py

# Com argumentos personalizados
uv run python metricas/gerar_performance.py --prompts 10 --output meus_resultados.csv --quiet
```

### Executar todas as m√©tricas (relat√≥rio consolidado)

```bash
# Executar todos os testes
uv run python metricas/run_all_metrics.py

# Pular testes espec√≠ficos
uv run python metricas/run_all_metrics.py --skip-performance --skip-quality
```

---

## üìä Scripts Dispon√≠veis

### 1. M√©tricas de Qualidade RAG

**Script:** `gerar_qualidade.py`

Executa consultas de teste no sistema RAG simulando requisi√ß√µes reais de usu√°rios.

**M√©tricas geradas:**

- Similaridade m√©dia e m√°xima por query
- Distribui√ß√£o de confian√ßa (ALTA, M√âDIA, BAIXA, SEM_RAG)
- Chunks recuperados por query
- Lat√™ncia de busca

**Argumentos CLI:**

```bash
uv run python metricas/gerar_qualidade.py [OPTIONS]

Options:
  --output, -o PATH      Caminho do CSV de sa√≠da (default: metricas/qualidade_rag.csv)
  --queries, -q INT      N√∫mero de queries a testar (default: 6)
  --verbose, -v          Modo verbose de logging
  --quiet, -q            Suprimir logs informativos
  --help                 Mostrar mensagem de ajuda
```

**Arquivos gerados:**

- `qualidade_rag.csv` - Dados brutos
- `qualidade_rag_summary.csv` - M√©tricas agregadas

---

### 2. M√©tricas de Performance End-to-End

**Script:** `gerar_performance.py`

Testa a lat√™ncia real de ponta a ponta do AgentWrapper para responder prompts no chat.

**M√©tricas geradas:**

- Tempo de resposta por prompt
- Comprimento da resposta gerada
- Uso de RAG (sim/n√£o)
- Taxa de sucesso
- Percentil 95 de lat√™ncia

**Argumentos CLI:**

```bash
uv run python metricas/gerar_performance.py [OPTIONS]

Options:
  --output, -o PATH      Caminho do CSV de sa√≠da (default: metricas/performance_geral.csv)
  --prompts, -p INT      N√∫mero de prompts a testar (default: 4)
  --verbose, -v          Modo verbose de logging
  --quiet, -q            Suprimir logs informativos
  --help                 Mostrar mensagem de ajuda
```

**Arquivos gerados:**

- `performance_geral.csv` - Dados brutos
- `performance_geral_summary.csv` - M√©tricas agregadas

---

### 3. M√©tricas de Performance de RAG (Componentes)

**Script:** `gerar_performance_rag.py`

Isola os componentes do RAG para medi√ß√£o individual.

**M√©tricas geradas:**

- Tempo de gera√ß√£o de embedding (OpenAI API)
- Tempo de busca vetorial (SQLite)
- Correla√ß√£o tamanho do texto ‚Üí tempo de embedding
- Chunks encontrados por busca

**Argumentos CLI:**

```bash
uv run python metricas/gerar_performance_rag.py [OPTIONS]

Options:
  --output, -o PATH      Caminho do CSV de sa√≠da (default: metricas/performance_rag_componentes.csv)
  --texts, -t INT        N√∫mero de textos a testar (default: 6)
  --verbose, -v          Modo verbose de logging
  --quiet, -q            Suprimir logs informativos
  --help                 Mostrar mensagem de ajuda
```

**Arquivos gerados:**

- `performance_rag_componentes.csv` - Dados brutos
- `performance_rag_componentes_summary.csv` - M√©tricas agregadas

---

### 4. M√©tricas de Performance de Acesso ao Banco

**Script:** `gerar_performance_acesso.py`

Executa opera√ß√µes massivas de escrita e leitura no SQLite para testar escalabilidade CRUD.

**M√©tricas geradas:**

- Throughput (opera√ß√µes/segundo)
- Lat√™ncia m√©dia de insert
- Lat√™ncia m√©dia de read
- Ratio insert vs read

**Argumentos CLI:**

```bash
uv run python metricas/gerar_performance_acesso.py [OPTIONS]

Options:
  --output, -o PATH      Caminho do CSV de sa√≠da (default: metricas/performance_acesso.csv)
  --inserts, -i INT      N√∫mero de opera√ß√µes de insert (default: 50)
  --reads, -r INT        N√∫mero de opera√ß√µes de read (default: 100)
  --verbose, -v          Modo verbose de logging
  --quiet, -q            Suprimir logs informativos
  --help                 Mostrar mensagem de ajuda
```

**Arquivos gerados:**

- `performance_acesso.csv` - Dados brutos
- `performance_acesso_summary.csv` - M√©tricas agregadas

---

### 5. Script Consolidado (Todas as M√©tricas)

**Script:** `run_all_metrics.py`

Executa todos os scripts de m√©tricas em sequ√™ncia e gera um relat√≥rio HTML consolidado.

**Funcionalidades:**

- Execu√ß√£o sequencial dos 4 scripts
- Relat√≥rio HTML com:
  - Cards de sum√°rio (total/sucesso/falha)
  - Gr√°ficos de barras CSS
  - Tabelas com dados
  - Status badges

**Argumentos CLI:**

```bash
uv run python metricas/run_all_metrics.py [OPTIONS]

Options:
  --skip-performance     Pular teste de performance end-to-end
  --skip-quality         Pular teste de qualidade RAG
  --skip-access          Pular teste de acesso ao banco
  --skip-rag             Pular teste de componentes RAG
  --help                 Mostrar mensagem de ajuda
```

**Arquivo gerado:**

- `relatorio_consolidado_<timestamp>.html` - Relat√≥rio visual completo

---

## üìà Formato dos Arquivos de Sa√≠da

### CSV Principal (dados brutos)

Cont√©m uma linha por medi√ß√£o com todos os campos coletados.

### CSV Summary (`_summary.csv`)

Cont√©m m√©tricas agregadas calculadas:

- M√©dias, medianas, percentis
- Distribui√ß√µes (porcentagens)
- Correla√ß√µes
- Throughput

### Console Output

Cada script exibe um sum√°rio estat√≠stico formatado no console ao final da execu√ß√£o:

```text
============================================================
SUM√ÅRIO ESTAT√çSTICO - PERFORMANCE ACESSO DB
============================================================
Throughput:                    792.18 ops/segundo

Compara√ß√£o Insert vs Read:
  Insert: 2.74ms avg (10 ops)
  Read:   0.52ms avg (20 ops)
  Ratio (Read/Insert):         0.19x

Total de opera√ß√µes:            30
Tempo total:                   0.038s
============================================================
```

---

## üéØ Exemplos de Uso

### Teste r√°pido (defaults)

```bash
uv run python metricas/run_all_metrics.py --skip-performance
```

### Teste completo com customiza√ß√£o

```bash
# Acesso ao banco - mais opera√ß√µes
uv run python metricas/gerar_performance_acesso.py --inserts 100 --reads 500

# RAG qualidade - mais queries
uv run python metricas/gerar_qualidade.py --queries 20 --output qualidade_extenso.csv
```

### Modo silencioso (para CI/CD)

```bash
uv run python metricas/run_all_metrics.py --quiet
```

### Ver detalhes com verbose

```bash
uv run python metricas/gerar_performance.py --verbose --prompts 2
```

---

## üìã Notas T√©cnicas

- Todos os scripts usam `asyncio` para opera√ß√µes ass√≠ncronas
- O banco de dados usa modo WAL para melhor concorr√™ncia
- Embeddings s√£o gerados via OpenAI API (text-embedding-3-small)
- Testes limpeza ap√≥s execu√ß√£o (delete de dados de teste)
- Formato de data/hora nos relat√≥rios: `YYYYMMDD_HHMMSS`

## üß™ Testes

### Executar testes unit√°rios

```bash
# Todos os testes
pytest metricas/tests/

# Testes espec√≠ficos
pytest metricas/tests/test_config.py
pytest metricas/tests/test_utils.py

# Com coverage
pytest metricas/tests/ --cov=metricas --cov-report=html

# Verbose
pytest metricas/tests/ -v
```

### Fixtures dispon√≠veis

- `temp_dir`: Diret√≥rio tempor√°rio para testes
- `sample_metrics_data`: Dados de exemplo
- `sample_csv_data`: CSV de exemplo
- `sample_html_template`: Template HTML de exemplo

## üîß Troubleshooting

### Erro: "No module named 'structlog'"

**Problema:** Depend√™ncia n√£o instalada.

**Solu√ß√£o:**
```bash
uv sync
```

### Erro: "Database file not found"

**Problema:** Banco de dados n√£o inicializado.

**Solu√ß√£o:**
```bash
# Executar migra√ß√µes
uv run alembic upgrade head
```

### Teste de qualidade retorna SEM_RAG para todas as queries

**Problema:** Threshold de similaridade muito alto ou embeddings n√£o gerados.

**Solu√ß√£o:**
```bash
# Verificar configura√ß√£o de min_similarity
# Valor padr√£o ajustado: 0.4 (baseado em dados emp√≠ricos)

# Reindexar embeddings se necess√°rio
uv run python scripts/ingest_legal_content.py
```

### Relat√≥rio HTML n√£o mostra estilos

**Problema:** Caminho do CSS relativo incorreto.

**Solu√ß√£o:**
- CSS deve estar em `metricas/static/report.css`
- HTML deve ter `<link rel="stylesheet" href="../static/report.css">`

### Timeout em scripts de m√©trica

**Problema:** Script demorando mais que o padr√£o (5 minutos).

**Solu√ß√£o:**
```bash
# Ajustar timeout em config.py ou passar par√¢metro
uv run python metricas/gerar_performance.py --prompts 5  # Menos prompts
```

## üìä M√©tricas e Interpreta√ß√£o

### Qualidade RAG

| Confian√ßa | Similaridade | Interpreta√ß√£o |
|-----------|--------------|---------------|
| ALTA | >= 0.70 | Conte√∫do muito relevante |
| M√âDIA | 0.55 - 0.70 | Conte√∫do relevante |
| BAIXA | 0.40 - 0.55 | Conte√∫do marginalmente relevante |
| SEM_RAG | < 0.40 | Nenhum conte√∫do relevante encontrado |

### Performance

| Lat√™ncia | Classifica√ß√£o |
|----------|---------------|
| < 1s | Excelente |
| 1s - 3s | Bom |
| 3s - 5s | Aceit√°vel |
| > 5s | Lento |

## üîÑ Migra√ß√£o de Scripts Legados

Se voc√™ tem scripts antigos de m√©trica, migre para a nova estrutura:

**Antes:**
```python
import asyncio
import csv
from pathlib import Path

async def my_metric():
    results = []
    # ... coleta de dados ...
    with open('output.csv', 'w') as f:
        writer = csv.DictWriter(f, fieldnames=['col1', 'col2'])
        writer.writerows(results)
```

**Depois:**
```python
from metricas.base_metric import BaseMetric, create_metric_cli
from metricas.utils import get_base_parser

class MyMetric(BaseMetric):
    def __init__(self):
        super().__init__(
            name='my_metric',
            description='Minha m√©trica customizada',
            output_file='my_metric.csv'
        )

    async def collect(self, **kwargs):
        results = []
        # ... coleta de dados ...
        return results

def main() -> None:
    create_metric_cli(MyMetric)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```
